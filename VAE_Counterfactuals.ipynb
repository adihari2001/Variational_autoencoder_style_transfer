{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_BNA0BUVzbJ"
      },
      "source": [
        "###Connecting to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpj24_7yV2VF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75748375-24a6-4306-c048-50a26627c423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_7P5rOwr2i8"
      },
      "source": [
        "##Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siTL8BfBYPtu"
      },
      "outputs": [],
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda x=False: \"UTF-8\"\n",
        "# !pip install alibi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QROv4cAF02l8",
        "outputId": "a2472271-e10a-413c-8f61-f05f53bcb600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import BCELoss, CrossEntropyLoss\n",
        "from torch.optim import Adam, RMSprop\n",
        "import torch.nn.functional as f\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import gc\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import csv\n",
        "import json\n",
        "import collections\n",
        "import logging\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
        "# from gensim.models import KeyedVectors\n",
        "from tqdm import tqdm\n",
        "use_cuda = True if torch.cuda.is_available() else False\n",
        "# from alibi.explainers import Counterfactual\n",
        "from torch.nn.modules import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm13242WiiSu",
        "outputId": "f6e905df-4307-4552-d16e-33870c294df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6-mJUc70i-I"
      },
      "source": [
        "##Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wx97P_7m6ei"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, text_file_path, labels_file_path, w2i_file_path, bow_file_path, i2w_file_path, mode='train'):\n",
        "        super(TextDataset, self).__init__()\n",
        "        # load train data\n",
        "\n",
        "        self.text_file_path = text_file_path\n",
        "        self.labels_file_path = labels_file_path\n",
        "        self.w2i_file_path = w2i_file_path\n",
        "        self.bow_file_path = bow_file_path\n",
        "        self.i2w_file_path = i2w_file_path\n",
        "        self.pad_token = 0\n",
        "        self.sos_token = 1\n",
        "        self.unk_token = 2\n",
        "        self.eos_token = 9189\n",
        "        self.max_seq_len = 15\n",
        "        self.bow_hidden_dim = 8229\n",
        "\n",
        "        with open(self.text_file_path) as f:\n",
        "            self.train_data = f.readlines()\n",
        "        # load train labels\n",
        "        with open(self.labels_file_path) as f:\n",
        "            self.train_labels = f.readlines()\n",
        "        # load word2index\n",
        "        with open(self.w2i_file_path) as f:\n",
        "            self.word2index = json.load(f)\n",
        "        #load index2word\n",
        "        with open(self.i2w_file_path) as f:\n",
        "            self.index2word = json.load(f)\n",
        "        # load bow vocab\n",
        "        with open(self.bow_file_path) as f:\n",
        "            self.bow_filtered_vocab_indices = json.load(f)\n",
        "        self.label2index = {'neg': [0, 1], 'pos': [1, 0]}\n",
        "        self.word2index['<eos>'] = 9189\n",
        "        self.index2word['9189'] = '<eos>'\n",
        "\n",
        "\n",
        "    def _padding(self, token_ids):\n",
        "        if len(token_ids) > self.max_seq_len:\n",
        "            return token_ids[:self.max_seq_len]\n",
        "        token_ids = token_ids + (self.max_seq_len-len(token_ids))*[self.pad_token]\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def _sentence_tokenid(self, sentence):\n",
        "        sentence = sentence + \"<eos>\"\n",
        "        token_ids = [self.word2index.get(word, self.unk_token) for word in sentence.split()]\n",
        "        padded_token_ids = self._padding(token_ids)\n",
        "        return padded_token_ids, len(token_ids)\n",
        "\n",
        "    def _get_bow_representations(self, text_sequence):\n",
        "        sequence_bow_representation = np.zeros(\n",
        "            shape=self.bow_hidden_dim, dtype=np.float32)\n",
        "        # Iterate over each word in the sequence\n",
        "        for index in text_sequence:\n",
        "            if index in self.bow_filtered_vocab_indices:\n",
        "                bow_index = self.bow_filtered_vocab_indices[index]\n",
        "                sequence_bow_representation[bow_index] += 1\n",
        "        sequence_bow_representation /= np.max(\n",
        "            [np.sum(sequence_bow_representation), 1])\n",
        "\n",
        "        return np.asarray(sequence_bow_representation) #8229 one hot\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.train_labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.train_data[index]\n",
        "        label = self.train_labels[index].strip()\n",
        "        label_final = self.label2index[label]\n",
        "        token_ids, seq_len = self._sentence_tokenid(sentence)\n",
        "        bow_rep = self._get_bow_representations(sentence)\n",
        "        sos_token = self.word2index.get(\"<sos>\") #1\n",
        "\n",
        "        data = dict()\n",
        "        data['bow'] = torch.FloatTensor(bow_rep)\n",
        "        data['label'] = torch.LongTensor(label_final) #[1,0] or [0,1]\n",
        "        data['input_sequence'] = torch.LongTensor(token_ids) #[tokens] size 15\n",
        "        data['sos_start'] = torch.LongTensor([sos_token]) #(batchsize, 1)\n",
        "\n",
        "        return data, sentence, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyY5KUoxtJPl"
      },
      "source": [
        "##Model Architecture - Variational AutoEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6lBveFC3y8K"
      },
      "source": [
        "####RNN -GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MMiYsq9sKRO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VAE_with_RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, weights_path, rnn_hidden_dim, rnn_hidden_dim_combined, style_hidden_dim, \n",
        "               content_hidden_dim, max_seq_length, embedding_size, bow_size, \n",
        "               concat_embedding_dim, num_styles,\n",
        "               dropout, lambdas, kl_params, batch_size, epochs, label_smooth):\n",
        "    super(VAE_with_RNN, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.rnn_hidden_dim = rnn_hidden_dim #256\n",
        "    self.rnn_hidden_dim_combined = rnn_hidden_dim_combined #512\n",
        "    self.content_hidden_dim = content_hidden_dim #128\n",
        "    self.style_hidden_dim = style_hidden_dim #8\n",
        "    self.epochs = epochs #20\n",
        "    self.label_smooth = label_smooth #0.1\n",
        "    self.max_seq_length = max_seq_length #15\n",
        "    self.batch_size = batch_size #128\n",
        "    self.bow_size = bow_size #8229\n",
        "    self.weights = torch.from_numpy(np.load(weights_path)) \n",
        "    #embedding weights\n",
        "\n",
        "    self.dropout_layer = nn.Dropout(p=dropout) #dropout\n",
        "    # self.transformer_encoder = nn.TransformerEncoderLayer(embedding_size, nhead=1, dim_feedforward=1024, batch_first=True)\n",
        "    # self.transformer_decoder = nn.TransformerDecoderLayer(embedding_size + style_hidden_dim + content_hidden_dim, nhead=1, dim_feedforward=1024, batch_first=True)\n",
        "    self.embedding = nn.Embedding(self.vocab_size, embedding_size).from_pretrained(self.weights) #9189\n",
        "    self.gru_encoder = nn.GRU(embedding_size, rnn_hidden_dim, bidirectional=True, batch_first=True)\n",
        "    self.style_mu = nn.Linear(rnn_hidden_dim_combined, style_hidden_dim)\n",
        "    self.style_sigma = nn.Linear(rnn_hidden_dim_combined, style_hidden_dim)\n",
        "    self.content_mu = nn.Linear(rnn_hidden_dim_combined, content_hidden_dim)\n",
        "    self.content_sigma = nn.Linear(rnn_hidden_dim_combined, content_hidden_dim)\n",
        "    # self.style_mu = nn.Linear(embedding_size, style_hidden_dim)\n",
        "    # self.style_sigma = nn.Linear(embedding_size, style_hidden_dim)\n",
        "    # self.content_mu = nn.Linear(embedding_size, content_hidden_dim)\n",
        "    # self.content_sigma = nn.Linear(embedding_size, content_hidden_dim)\n",
        "    self.style_adv = nn.Linear(content_hidden_dim, num_styles) #content to 2\n",
        "    self.content_adv = nn.Linear(style_hidden_dim, bow_size) \n",
        "    self.content_classifer = nn.Linear(content_hidden_dim, bow_size)\n",
        "    self.style_classifer = nn.Linear(style_hidden_dim, num_styles)\n",
        "    # self.enc_agg = nn.Linear(15*300,300)\n",
        "    # self.enc_feed = nn.Linear(300,436)\n",
        "\n",
        "    self.latent_to_hidden = nn.Linear(concat_embedding_dim, rnn_hidden_dim) #hidden state dim\n",
        "    self.decoder = nn.GRU(embedding_size + style_hidden_dim + content_hidden_dim, rnn_hidden_dim, bidirectional = False, batch_first=True) # \n",
        "    self.vocab_project = nn.Linear(rnn_hidden_dim, self.vocab_size)\n",
        "    # self.vocab_project = nn.Linear(300, self.vocab_size)\n",
        "    self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "    self.avg_style_emb = {\n",
        "            0: torch.zeros(style_hidden_dim).to(device),\n",
        "            1: torch.zeros(style_hidden_dim).to(device)\n",
        "        }\n",
        "    self.num_neg_samples = 0\n",
        "    self.num_pos_samples = 0    \n",
        " \n",
        "  def forward(self, data, train_numbers=None):\n",
        "    input_sequence = data['input_sequence'].to(device) #sequence of tokens\n",
        "    self.batch_size = input_sequence.shape[0]\n",
        "    sos_starting_sequence = data['sos_start'].to(device)\n",
        "    labels = data['label'].to(device) #already repeated and then sent inside\n",
        "    iteration = train_numbers['iteration']\n",
        "    current_epoch = train_numbers['epoch']\n",
        "    # input_encoder = torch.cat((sos_starting_sequence, input_sequence), dim=1).long()\n",
        "    embeddings = self.embedding(input_sequence)#.to(torch.long))\n",
        "    #embeddings = embeddings.tolist()\n",
        "    encoder_hidden_states, _ = self.gru_encoder(embeddings)\n",
        "    # encoder_hidden_states = self.transformer_encoder(embeddings) #bs,16,256\n",
        "    # encoder_decoder_hidden_states = self.enc_feed(encoder_hidden_states)\n",
        "    final_hidden_state = encoder_hidden_states[:,-1,:]\n",
        "    # final_hidden_state = encoder_hidden_states.reshape(self.batch_size, -1)\n",
        "    # final_hidden_state = self.enc_agg(final_hidden_state)\n",
        "    \n",
        "    style_mu = self.dropout_layer(self.style_mu(final_hidden_state))\n",
        "    style_sigma = self.dropout_layer(self.style_sigma(final_hidden_state))\n",
        "    content_mu = self.dropout_layer(self.content_mu(final_hidden_state))\n",
        "    content_sigma = self.dropout_layer(self.content_sigma(final_hidden_state))\n",
        "\n",
        "    content_sample = torch.randn(self.content_hidden_dim).to(device)\n",
        "    style_sample = torch.randn(self.style_hidden_dim).to(device)\n",
        "    \n",
        "    content_sample = content_mu +  content_sample*torch.exp(content_sigma)\n",
        "    style_sample = style_mu +  style_sample*torch.exp(style_sigma)\n",
        "    concat_embedding = torch.cat((style_sample, content_sample), axis=1) #bs,436\n",
        "    if current_epoch==self.epochs:\n",
        "      for idx, label in enumerate(labels):\n",
        "        if label[0] == 0:\n",
        "          self.num_neg_samples+=1\n",
        "          self.avg_style_emb[0] = (self.avg_style_emb[0]*(self.num_neg_samples-1) + style_sample[idx])/self.num_neg_samples\n",
        "        elif label[0] == 1:\n",
        "          self.num_pos_samples+=1\n",
        "          self.avg_style_emb[1] = (self.avg_style_emb[1]*(self.num_pos_samples-1) + style_sample[idx])/self.num_pos_samples\n",
        "    #content side preds\n",
        "    style_disc_preds = nn.Softmax(dim=1)(self.dropout_layer(self.style_adv(content_sample.detach())))\n",
        "    content_mul_preds = nn.Softmax(dim=1)(self.dropout_layer(self.content_classifer(content_sample)))\n",
        "    #style side preds\n",
        "    content_disc_preds = nn.Softmax(dim=1)(self.dropout_layer(self.content_adv(style_sample.detach())))\n",
        "    style_mul_preds = nn.Softmax(dim=1)(self.dropout_layer(self.style_classifer(style_sample)))\n",
        "    \n",
        "    #reconstruction_process and recon loss\n",
        "    #without teacher forcing - autoregressive\n",
        "    # output_tokens = torch.zeros(self.max_seq_length, self.batch_size).to(device)\n",
        "    # output_dist = torch.zeros(self.max_seq_length, self.batch_size, self.vocab_size).to(device)\n",
        "    # decoder_input = sos_starting_sequence.long().to(device) #bs,1\n",
        "    # # decoder_input = sos_starting_sequence.long().to(device).repeat(1,self.max_seq_length+1)\n",
        "    # decoder_input_embedding = self.dropout_layer(self.embedding(decoder_input)) #bs,1,300\n",
        "    # decoder_input_embedding = torch.cat((decoder_input_embedding,concat_embedding.unsqueeze(1)),dim=2) #bs,1,436\n",
        "    # hidden_state = self.latent_to_hidden(concat_embedding).unsqueeze(1).permute(1,0,2).contiguous() #1,bs,256\n",
        "\n",
        "    # # generating tokens autoregressively\n",
        "    # for i in range(self.max_seq_length):\n",
        "    #   rnn_out, hidden_state = self.decoder(decoder_input_embedding, hidden_state) \n",
        "    #   # print(rnn_out.shape, \"rnn\")\n",
        "\n",
        "    #   output_dist[i] = self.vocab_project(rnn_out[:,-1,:]) #bs,9190\n",
        "    #   # print(output_dist.shape, 'output dist')\n",
        "    #   output_dist[i] = self.softmax(output_dist[i])\n",
        "    #   output_tokens[i] = output_dist[i].argmax(-1)\n",
        "    #   # print(output_tokens.shape, 'output tokens')\n",
        "    #   # print(self.dropout_layer(self.embedding(output_tokens[i].long())).shape, 'embedding')\n",
        "    #   # print(concat_embedding.shape, 'concat')\n",
        "    #   next_token_append = torch.cat((self.dropout_layer(self.embedding(output_tokens[i].long())), concat_embedding), dim=1)\n",
        "    #   # print(next_token_append.shape, 'next') #bs,1,436\n",
        "    #   decoder_input_embedding = torch.cat((decoder_input_embedding, next_token_append.unsqueeze(1)),dim=1) #\n",
        "    #   # decoder_input_embedding[:,i] = next_token_append\n",
        "    #   # print(decoder_input.shape, 'di')\n",
        "\n",
        "    # output_dist = output_dist.permute(1,0,2)\n",
        "    # output_tokens = output_tokens.permute(1,0)\n",
        "\n",
        "    #with teacher forcing\n",
        "    decoder_input = torch.cat((sos_starting_sequence, input_sequence), dim=1).long() #(bs,16)\n",
        "    decoder_sentence_embedding = self.dropout_layer(self.embedding(decoder_input)) #(bs,16,300)\n",
        "    decoder_sentence_embedding = torch.cat((decoder_sentence_embedding, concat_embedding.unsqueeze(1).repeat(1,self.max_seq_length+1,1)), dim=2) #(bs,16,436)\n",
        "    hidden_state = self.latent_to_hidden(concat_embedding).unsqueeze(1).permute(1,0,2).contiguous() #unsqueeze(1).permute(1,0,2).repeat(2,1,1)\n",
        "    output, hidden_state = self.decoder(decoder_sentence_embedding, hidden_state) #bs, 15, 256\n",
        "    output_dist = self.vocab_project(output)[:,:-1]\n",
        "    output_tokens = self.logsoftmax(output_dist).argmax(-1)\n",
        "    output_tokens = output_tokens[:,:-1]\n",
        "  \n",
        "\n",
        "    return output_dist, output_tokens, style_mu, style_sigma, content_mu, content_sigma, \\\n",
        "    content_disc_preds, style_disc_preds, content_mul_preds, style_mul_preds, \\\n",
        "    content_sample, style_sample, self.avg_style_emb, input_sequence, labels\n",
        "    \n",
        "    #if style_transfer during testing\n",
        "    #generates autoregressively\n",
        "    #could be either just reconstruction, style_transfer, cf_style_transfer\n",
        "  def inference(self, data, outtype='reconstruction', target_style_sample=None, cf=None):\n",
        "    input_sequence = data['input_sequence'].to(device)\n",
        "    sos_starting_sequence = data['sos_start'].to(device) #already repeated and then sent inside\n",
        "    labels = data['label'].to(device)\n",
        "    bow_feat = data['bow'].to(device)\n",
        "    self.batch_size = labels.shape[0]\n",
        "    embeddings = self.embedding(input_sequence)\n",
        "    encoder_hidden_states, h_n = self.gru_encoder(embeddings)\n",
        "    # print(encoder_hidden_states.shape)\n",
        "    final_hidden_state = encoder_hidden_states[:,-1]\n",
        "    content_mu = self.content_mu(final_hidden_state)\n",
        "    content_sigma = self.content_sigma(final_hidden_state)\n",
        "    style_mu = self.style_mu(final_hidden_state)\n",
        "    style_sigma = self.style_sigma(final_hidden_state)\n",
        "    content_sample = content_mu +  torch.randn(self.content_hidden_dim).to(device)*torch.exp(content_sigma)\n",
        "    style_sample = style_mu +  torch.randn(self.style_hidden_dim).to(device)*torch.exp(style_sigma)\n",
        "    if outtype=='reconstruction':\n",
        "      concat_embedding = torch.cat((style_sample, content_sample), dim=-1) \n",
        "\n",
        "    elif outtype == 'style' and target_style_sample is not None:\n",
        "      if labels[0][0]==0:\n",
        "        target_emb = torch.Tensor(target_style_sample[1]).unsqueeze(0).to(device)\n",
        "      elif labels[0][0]==1:\n",
        "        target_emb = torch.Tensor(target_style_sample[0]).unsqueeze(0).to(device)\n",
        "      concat_embedding = torch.cat((target_emb, content_sample), dim=-1)\n",
        "\n",
        "    elif outtype == 'cf_style' and cf is not None: #bs, 8\n",
        "      #pass style sample to .explain\n",
        "      #data cf x\n",
        "      cf_data = cf.explain(style_sample.to('cpu').reshape(1,8).numpy().astype(np.float64))\n",
        "      cf_style_sample = torch.Tensor(cf_data['data']['cf']['X']).to(device)\n",
        "      concat_embedding = torch.cat((cf_style_sample, content_sample), dim=-1)\n",
        "    \n",
        "    style_disc_preds = nn.Softmax(dim=1)(self.style_adv(content_sample.detach()))\n",
        "    content_mul_preds = nn.Softmax(dim=1)(self.content_classifer(content_sample))\n",
        "    #style side preds\n",
        "    content_disc_preds = nn.Softmax(dim=1)(self.content_adv(style_sample.detach()))\n",
        "    style_mul_preds = nn.Softmax(dim=1)(self.style_classifer(style_sample))\n",
        "      \n",
        "\n",
        "    #reconstruction_process and recon loss\n",
        "    output_tokens = torch.zeros(self.max_seq_length, self.batch_size).to(device)\n",
        "    output_dist = torch.zeros(self.max_seq_length, self.batch_size, self.vocab_size).to(device)\n",
        "    decoder_input = sos_starting_sequence.long().to(device) #bs,1\n",
        "    # decoder_input = sos_starting_sequence.long().to(device)\n",
        "    decoder_input_embedding = self.embedding(decoder_input) #bs,1,300\n",
        "    decoder_input_embedding = torch.cat((decoder_input_embedding,concat_embedding.unsqueeze(1)),dim=2) #bs,1,436\n",
        "    hidden_state = self.latent_to_hidden(concat_embedding).unsqueeze(1).permute(1,0,2).contiguous() #1,bs,256\n",
        "\n",
        "    # generating tokens autoregressively\n",
        "    for i in range(self.max_seq_length):\n",
        "      rnn_out, hidden_state = self.decoder(decoder_input_embedding, hidden_state) \n",
        "      output_dist[i] = self.vocab_project(rnn_out[:,-1,:])\n",
        "      output_tokens[i] = self.softmax(self.vocab_project(rnn_out[:,-1,:])).argmax(-1) #bs,9190\n",
        "      next_token_append = torch.cat((self.embedding(output_tokens[i].long()), concat_embedding), dim=1)\n",
        "      decoder_input_embedding = torch.cat((decoder_input_embedding, next_token_append.unsqueeze(1)),dim=1) \n",
        "\n",
        "    # output_dist = output_dist.permute(1,0,2)\n",
        "    output_tokens = output_tokens.permute(1,0)\n",
        "    output_dist = output_dist.permute(1,0,2) #bs,15\n",
        "\n",
        "      #(batchsize, 1, 436) decoder changing \n",
        "      # batchsize, 2, 436) rnn out= bs, 2, 512 --> vocab project -->token\n",
        "      #token --> embedding(token) - concat --> # (bs,3,436)\n",
        "    return output_dist, output_tokens, style_mu, style_sigma, content_mu, content_sigma, \\\n",
        "    content_disc_preds, style_disc_preds, content_mul_preds, style_mul_preds, \\\n",
        "    content_sample, style_sample, self.avg_style_emb, input_sequence, labels #max_seq_length,batch_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x4uDZN35Ht1"
      },
      "source": [
        "##CounterFactual Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-6WUPiN5GRe"
      },
      "outputs": [],
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
        "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.fc3 = nn.Linear(hidden_size_2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.nn.functional.relu(self.fc3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTWWXK5ByzLb"
      },
      "source": [
        "##Experiments, Train and Validate functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To-COgKfnM9m"
      },
      "source": [
        "###Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrYKxGEnQt7B"
      },
      "outputs": [],
      "source": [
        "class Experiment():\n",
        "  def __init__(self, train_dataset, params, \n",
        "               hyperparams, avg_emb_save_path, logging_path, epoch_logging_path,\n",
        "               model_save_path, previous_save, print_batch, \n",
        "               val_dataset, test_dataset, word2indexpath, index2wordpath):\n",
        "    # self.train_logger= train_logger\n",
        "    self.model_hyperparams = params\n",
        "    # self.train_logger = logging.getLogger('train_logger')\n",
        "    self.hyperparams = hyperparams\n",
        "    self.logging_path = logging_path\n",
        "    self.batch_size = self.model_hyperparams['batch_size']\n",
        "    self.num_epochs = self.model_hyperparams['epochs']\n",
        "    self.model = VAE_with_RNN(**self.model_hyperparams)\n",
        "    self.model = self.model.double().to(device)\n",
        "    self.model_save_path = model_save_path\n",
        "    self.print_batch = print_batch\n",
        "    self.train_dataset = train_dataset\n",
        "    self.val_dataset = val_dataset\n",
        "    self.test_dataset = test_dataset\n",
        "    self.train_dataloader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
        "    self.val_dataloader = DataLoader(self.val_dataset, self.batch_size, shuffle=True)\n",
        "    self.test_dataloader = DataLoader(self.test_dataset, self.batch_size, shuffle=False)\n",
        "    self.avg_emb_save_path = avg_emb_save_path\n",
        "    self.model_save_path = model_save_path\n",
        "    self.previous_save = previous_save\n",
        "    self.epoch_logging_path = epoch_logging_path\n",
        "    self.index2word = json.load(open(index2wordpath,'r'))\n",
        "    self.word2index = json.load(open(word2indexpath,'r'))\n",
        "    self.index2word['9189'] = '<eos>'\n",
        "    self.word2index['<eos>'] = 9189\n",
        "    self.counterfactual_mlp = MLP(8,32,32,2)\n",
        "    cf_checkpoint = \"/content/drive/MyDrive/NLPPROJECT/Models/mlp_10\"\n",
        "    checkpoint = torch.load(cf_checkpoint)\n",
        "    self.counterfactual_mlp.load_state_dict(checkpoint['model_state_dict'])\n",
        "    self.counterfactual_mlp.eval()\n",
        "    \n",
        "    self.main_opt = torch.optim.Adam(self.model.parameters(), lr=self.hyperparams['lrs']['encoder_classifier_lr'])\n",
        "\n",
        "\n",
        "  def init_counterfactual(self):\n",
        "    def model_fn(x):  # set the model to evaluation mode\n",
        "      with torch.no_grad():\n",
        "          x = torch.from_numpy(x).to(torch.float32)\n",
        "          # print(x.shape, x.dtype)\n",
        "          y_pred = torch.nn.functional.softmax(self.counterfactual_mlp(x), dim=1).numpy().astype(np.float64)  # use the model to predict labels\n",
        "      return y_pred\n",
        "    \n",
        "    cf = Counterfactual(model_fn, shape=(1, 8), target_proba=1.0, target_class=\"other\", lam_init=0.1)\n",
        "    \n",
        "    return cf\n",
        "  \n",
        "\n",
        "    # self.dataloader = Dataloader(dataset)\n",
        "\n",
        "  def label_smooth(self,label_smooth, data, style=True, num_styles=2, bow_size=8229):\n",
        "    if style==True:\n",
        "      return data* (1-label_smooth) + label_smooth/num_styles\n",
        "    else:\n",
        "      return data* (1-label_smooth) + label_smooth/bow_size\n",
        "\n",
        "  def kl_loss(self,sigma, mu):\n",
        "    return torch.mean((-0.5*torch.sum(1+sigma - sigma.exp()-mu.pow(2), dim=1)))\n",
        "\n",
        "  def disc_loss(self,disc_preds, smoothed_labels):\n",
        "    return nn.BCELoss()(disc_preds, smoothed_labels)\n",
        "\n",
        "  def entropy_loss(self,disc_preds,eps):\n",
        "    return torch.mean(torch.sum(-disc_preds * torch.log(disc_preds + eps), dim=1))\n",
        "\n",
        "  def mul_loss(self,mul_preds, smoothed_labels):\n",
        "    return nn.BCELoss()(mul_preds, smoothed_labels)\n",
        "\n",
        "  def recon_loss(self,output_prob, gt_tokens):\n",
        "    return nn.CrossEntropyLoss(ignore_index=0)(output_prob.contiguous().view(-1, self.model_hyperparams['vocab_size']), gt_tokens.view(-1))\n",
        "\n",
        "    \n",
        "  def train(self):\n",
        "    \n",
        "    training_numbers = dict()\n",
        "    training_numbers['iteration'] = 1\n",
        "    training_numbers['epoch'] = 1\n",
        "    # if len(os.listdir(self.model_save_path))!=0:\n",
        "    #   load_path = os.path.join(self.model_save_path, \"model_epoch\"+str(self.previous_save)+\".pts\")\n",
        "    #   checkpoint = torch.load(load_path)\n",
        "    #   self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #   self.style_adv_opt.load_state_dict(checkpoint['style_opt_state_dict'])\n",
        "    #   self.content_adv_opt.load_state_dict(checkpoint['style_opt_state_dict'])\n",
        "    #   self.style_adv_opt.load_state_dict(checkpoint['style_opt_state_dict'])\n",
        "    #   self.start_epoch = checkpoint['epoch']\n",
        "    #   print('Model Loaded from Checkpoint')\n",
        "    # else:\n",
        "    #   self.start_epoch = 1\n",
        "    #   print('New Model Initialized')\n",
        "\n",
        "    self.model.train()\n",
        "    # self.train_logger.info(\"Experiment Created, Training Started!\")\n",
        "    print('Training Started!')\n",
        "    print(self.model.embedding)\n",
        "    # log_path = save_path + \n",
        "    e_sal = list()\n",
        "    e_cal = list()\n",
        "    e_sml = list()\n",
        "    e_cml = list()\n",
        "    e_rl = list()\n",
        "    e_kl = list()\n",
        "    e_se = list()\n",
        "    e_ce = list()\n",
        "    e_ol = list()\n",
        "    e_saacc = list()\n",
        "    e_smacc = list()\n",
        "    loss_acc = dict()\n",
        "    val_losses_dict = {'e_sal':[], 'e_cal':[], 'e_sml':[], 'e_cml':[], 'e_rl':[], 'e_kl':[], 'e_ol':[], 'e_se':[], 'e_ce':[]}\n",
        "    with open(os.path.join(self.logging_path,'train_logger_model_new.txt'),'w') as file:\n",
        "      for epoch in range(1,self.num_epochs+1):\n",
        "        running_style_adv_loss = 0.0\n",
        "        running_content_adv_loss = 0.0\n",
        "        running_over_loss = 0.0\n",
        "        running_recon_loss = 0.0\n",
        "        running_content_mul_loss = 0.0\n",
        "        running_style_mul_loss = 0.0\n",
        "        running_kl_loss = 0.0\n",
        "        running_content_entropy = 0.0\n",
        "        running_style_entropy = 0.0\n",
        "        running_style_adv_acc = 0.0\n",
        "        running_style_mul_acc = 0.0\n",
        "\n",
        "        \n",
        "        for idx, data in enumerate(tqdm(self.train_dataloader)):\n",
        "          #data['input_sequence'] = torch.DoubleTensor(data['input_sequence'])\n",
        "          #data['input_sequence'] = data['input_sequence'].to(torch.double)\n",
        "          data, sentence, label = data\n",
        "          output_tuple = self.model(data,training_numbers)\n",
        "          # output_tuple = output_tuple.to('cpu')\n",
        "          output_dist = output_tuple[0] \n",
        "          output_tokens = output_tuple[1] \n",
        "          style_mu = output_tuple[2]\n",
        "          style_sigma = output_tuple[3]\n",
        "          content_mu = output_tuple[4]\n",
        "          content_sigma = output_tuple[5]\n",
        "          content_disc_preds = output_tuple[6]\n",
        "          style_disc_preds = output_tuple[7]\n",
        "          content_mul_preds = output_tuple[8] \n",
        "          style_mul_preds = output_tuple[9] \n",
        "          content_sample = output_tuple[10]\n",
        "          style_sample = output_tuple[11]\n",
        "          input_sequence = output_tuple[13]\n",
        "          labels = output_tuple[14]\n",
        "\n",
        "          bow_feat = data['bow']\n",
        "          # labels = data['label']\n",
        "\n",
        "          smoothed_bow = self.label_smooth(self.hyperparams['label_smooth'], bow_feat, False, self.model_hyperparams['bow_size']).to(torch.float32).to(device)\n",
        "          smoothed_labels = self.label_smooth(self.hyperparams['label_smooth'], labels, True).to(device)\n",
        "\n",
        "          content_disc_loss = self.disc_loss(content_disc_preds, smoothed_bow.double()).sum()\n",
        "          style_disc_loss = self.disc_loss(style_disc_preds, smoothed_labels.double()).sum()\n",
        "\n",
        "          content_entropy_loss = self.entropy_loss(content_disc_preds, self.hyperparams['eps']).sum()\n",
        "          style_entropy_loss = self.entropy_loss(style_disc_preds, self.hyperparams['eps']).sum()\n",
        "          \n",
        "          content_kl_loss = self.kl_loss(content_sigma, content_mu)\n",
        "          style_kl_loss = self.kl_loss(style_sigma, style_mu)\n",
        "          kl_loss = style_kl_loss.item() + content_kl_loss.item()\n",
        "\n",
        "          content_mul_loss = self.mul_loss(content_mul_preds, smoothed_bow.double()).sum()\n",
        "          style_mul_loss = self.mul_loss(style_mul_preds, smoothed_labels.double()).sum()\n",
        "\n",
        "\n",
        "          reconstruction_loss = self.recon_loss(output_dist, input_sequence).sum()\n",
        "\n",
        "          it = training_numbers['iteration']\n",
        "\n",
        "          lambdas = self.model_hyperparams['lambdas']\n",
        "          kl_params = self.model_hyperparams['kl_params']\n",
        "          if it < kl_params[\"kl_max_iter\"]:\n",
        "            kl_params['kl_weight'] = ((math.tanh((it - kl_params[\"kl_max_iter\"] * 1.5) / (kl_params[\"kl_max_iter\"] / 3))+ 1) * kl_params[\"kl_weight\"])\n",
        "\n",
        "          ovr_loss = reconstruction_loss + kl_params['kl_weight']*kl_loss + lambdas['lam_style_mul']*style_mul_loss + lambdas['lam_content_mul']*content_mul_loss - lambdas['lam_style_adv']*style_entropy_loss - lambdas['lam_content_adv']*content_entropy_loss\n",
        "\n",
        "          self.main_opt.zero_grad()\n",
        "\n",
        "          style_disc_loss.backward(retain_graph=True)\n",
        "          content_disc_loss.backward(retain_graph=True)\n",
        "          ovr_loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(self.model.parameters(), 3)\n",
        "          self.main_opt.step()\n",
        "\n",
        "          current_batch_size = labels.shape[0]\n",
        "\n",
        "          running_style_adv_loss += style_disc_loss.item()\n",
        "          running_content_adv_loss += content_disc_loss.item()\n",
        "          running_over_loss += ovr_loss.item()\n",
        "          running_recon_loss+= reconstruction_loss.item()\n",
        "          running_content_entropy += content_entropy_loss.item()\n",
        "          running_style_entropy += style_entropy_loss.item()\n",
        "          running_kl_loss += kl_loss\n",
        "          running_content_mul_loss += content_mul_loss.item()\n",
        "          running_style_mul_loss += style_mul_loss.item()\n",
        "\n",
        "\n",
        "          style_mul_labels = torch.where(style_mul_preds>0.5,1,0)\n",
        "          style_adv_labels = torch.where(style_disc_preds>0.5,1,0)\n",
        "          style_mul_acc = (style_mul_labels==labels).sum()/(2*current_batch_size)\n",
        "          style_adv_acc = (style_adv_labels==labels).sum()/(2*current_batch_size)\n",
        "\n",
        "          running_style_adv_acc += style_adv_acc\n",
        "          running_style_mul_acc += style_mul_acc\n",
        "\n",
        "          if it % self.print_batch == 0 or it+1 == len(self.train_dataloader):\n",
        "\n",
        "\n",
        "            # print(\"-----------------------------------------------------------------------\")\n",
        "\n",
        "            output_string = \"Epoch %i\\n Iteration %i\\n Overall Loss %9.4f\\n Recon_Loss %9.4f \\n KL-Loss %9.5f\\n Style-Mul-Loss %9.4f\\n Content-Mul-Loss %9.4f\\n Style-Adv-Entropy %9.4f\\n Content-Adv-Entropy %9.4f\\n Content-disc-loss %9.4f\\n Style-disc-loss %9.4f\\n Style Multi Task Accuracy %9.4f\\n Style Discriminator Accurary %9.4f\\n\" % (epoch, it, running_over_loss/(idx+1), running_recon_loss/(idx+1), kl_params['kl_weight']*running_kl_loss/(idx+1), \n",
        "            running_style_mul_loss/(idx+1), running_content_mul_loss/(idx+1), running_style_entropy/(idx+1), running_content_entropy/(idx+1), running_content_adv_loss/(idx+1), running_style_adv_loss/(idx+1), running_style_mul_acc/(idx+1), running_style_adv_acc/(idx+1))\n",
        "            print(output_string)\n",
        "\n",
        "\n",
        "            file.write(output_string)\n",
        "            file.write('------------------------------------------------------------------')\n",
        "\n",
        "            torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': self.model.state_dict(),\n",
        "              'main_opt_state_dict': self.main_opt.state_dict()\n",
        "              }, os.path.join(self.model_save_path,\"model_epoch_teacherforce_%i\"%(epoch)+\".pt\"))\n",
        "\n",
        "            self.previous_save = epoch\n",
        "\n",
        "          #printing every 100 iterations\n",
        "          training_numbers['iteration']+=1\n",
        "\n",
        "       \n",
        "        e_sal.append(float(running_style_adv_loss/(idx+1)))\n",
        "        e_cal.append(float(running_content_adv_loss/(idx+1)))\n",
        "        e_sml.append(float(running_style_mul_loss/(idx+1)))\n",
        "        e_cml.append(float(running_content_mul_loss/(idx+1)))\n",
        "        e_rl.append(float(running_recon_loss/(idx+1)))\n",
        "        e_kl.append(float(running_kl_loss/(idx+1)))\n",
        "        e_se.append(float(running_style_entropy/(idx+1)))\n",
        "        e_ce.append(float(running_style_entropy/(idx+1)))\n",
        "        e_saacc.append(float(running_style_adv_acc/(idx+1)))\n",
        "        e_smacc.append(float(running_style_mul_acc/(idx+1)))\n",
        "        e_ol.append(float(running_over_loss/(idx+1)))\n",
        "\n",
        "        loss_acc['St_Mul_L'] = e_sal \n",
        "        loss_acc['Co_Mul_L'] = e_cal \n",
        "        loss_acc['St_Adv_L'] = e_sml \n",
        "        loss_acc['Co_Adv_L'] = e_cml \n",
        "        loss_acc['Recon_L'] = e_rl \n",
        "        loss_acc['KL_L'] = e_kl \n",
        "        loss_acc['Over_L'] = e_ol \n",
        "        loss_acc['St_Ent'] = e_se \n",
        "        loss_acc['Co_Ent'] = e_ce \n",
        "        loss_acc['St_Mul_'] = e_smacc \n",
        "        loss_acc['St_Adv_Acc'] = e_saacc \n",
        "\n",
        "        with open(os.path.join(self.epoch_logging_path, \"TrainLosses.json\"),'w') as final:\n",
        "          json.dump(loss_acc, final)\n",
        "\n",
        "        with open(os.path.join(self.epoch_logging_path, \"ValLosses.json\"),'w') as final_val:\n",
        "          json.dump(val_losses_dict,final_val)\n",
        "\n",
        "        # print(\"-----------------------------------------------------------------------\")\n",
        "        # epoch_string = \"Epoch %i\\n Iteration %i\\n Overall Loss %9.4f\\n Recon_Loss %9.4f \\n KL-Loss %9.5f\\n Style-Mul-Loss %9.4f\\n Content-Mul-Loss %9.4f\\n Style-Adv-Entropy %9.4f\\n Content-Adv-Entropy %9.4f\\n Content-disc-loss %9.4f\\n Style-disc-loss %9.4f\\n \" % (epoch, it, running_over_loss/idx, running_recon_loss/idx, running_kl_loss/idx, \n",
        "        # running_style_mul_loss/idx, running_content_mul_loss/idx, running_style_entropy/idx, running_content_entropy/idx, running_content_adv_loss/idx, running_style_adv_loss/idx)\n",
        "        # print(epoch_string)\n",
        "        # file.write(epoch_string)\n",
        "        # file.write('---------------------------------------------------------------------------------')\n",
        "        # file.write('---------------------------------------------------------------------------------')\n",
        "\n",
        "        val_losses_dict, _, _, _ = self.validate(self.val_dataloader, val_losses_dict, test=False)\n",
        "        self.model.train()\n",
        "        \n",
        "\n",
        "        if epoch == self.num_epochs:\n",
        "          avg_neg_style_emb = output_tuple[12][0].unsqueeze(0)\n",
        "          avg_pos_style_emb = output_tuple[12][1].unsqueeze(0)\n",
        "          combined = torch.cat((avg_neg_style_emb, avg_pos_style_emb), dim=0).cpu().detach().numpy()\n",
        "          np.save(os.path.join(self.avg_emb_save_path,\"avg_emb.npy\"), combined)\n",
        "        \n",
        "        training_numbers['epoch']+=1  \n",
        "      \n",
        "      file.close()\n",
        "\n",
        "      return self.previous_save\n",
        "  \n",
        "  def validate(self, dataloader, val_losses_dict,test=False): #returns validation losses\n",
        "    all_output_tokens = torch.Tensor().to(device)\n",
        "    all_labels = list()\n",
        "    all_sentences = list()\n",
        "    if test:\n",
        "      log_file_path = 'test_logger_new.txt'\n",
        "    else:\n",
        "      log_file_path = 'val_logger_new.txt'\n",
        "    with open(os.path.join(self.logging_path,log_file_path),'a+') as val_file:\n",
        "      with torch.no_grad():\n",
        "        running_style_adv_loss = 0.0\n",
        "        running_content_adv_loss = 0.0\n",
        "        running_over_loss = 0.0\n",
        "        running_recon_loss = 0.0\n",
        "        running_content_mul_loss = 0.0\n",
        "        running_style_mul_loss = 0.0\n",
        "        running_kl_loss = 0.0\n",
        "        running_content_entropy = 0.0\n",
        "        running_style_entropy = 0.0\n",
        "        for idx, data in enumerate(dataloader):\n",
        "          #data['input_sequence'] = torch.DoubleTensor(data['input_sequence'])\n",
        "          #data['input_sequence'] = data['input_sequence'].to(torch.double)\n",
        "          data, sentence, label = data\n",
        "          self.batch_size = data['label'].shape[0]\n",
        "          all_sentences.extend(sentence)\n",
        "          all_labels.extend(label)\n",
        "          output_tuple = self.model.inference(data,outtype='reconstruction')\n",
        "          # output_tuple = output_tuple.to('cpu')\n",
        "          output_dist = output_tuple[0]\n",
        "          output_tokens = output_tuple[1] \n",
        "          \n",
        "          style_mu = output_tuple[2]\n",
        "          style_sigma = output_tuple[3]\n",
        "          content_mu = output_tuple[4]\n",
        "          content_sigma = output_tuple[5]\n",
        "          content_disc_preds = output_tuple[6]\n",
        "          style_disc_preds = output_tuple[7]\n",
        "          content_mul_preds = output_tuple[8] \n",
        "          style_mul_preds = output_tuple[9] \n",
        "          content_sample = output_tuple[10]\n",
        "          style_sample = output_tuple[11]\n",
        "          input_sequence = output_tuple[13]\n",
        "          labels = output_tuple[14]\n",
        "\n",
        "          all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
        "\n",
        "          bow_feat = data['bow']\n",
        "          # labels = data['label']\n",
        "\n",
        "          smoothed_bow = self.label_smooth(self.hyperparams['label_smooth'], bow_feat, False, self.model_hyperparams['bow_size']).to(torch.float32).to(device)\n",
        "          smoothed_labels = self.label_smooth(self.hyperparams['label_smooth'], labels, True).to(device)\n",
        "\n",
        "          content_disc_loss = self.disc_loss(content_disc_preds, smoothed_bow.double()).sum()\n",
        "          style_disc_loss = self.disc_loss(style_disc_preds, smoothed_labels.double()).sum()\n",
        "\n",
        "          content_entropy_loss = self.entropy_loss(content_disc_preds, self.hyperparams['eps']).sum()\n",
        "          style_entropy_loss = self.entropy_loss(style_disc_preds, self.hyperparams['eps']).sum()\n",
        "          \n",
        "          content_kl_loss = self.kl_loss(content_sigma, content_mu).sum()\n",
        "          style_kl_loss = self.kl_loss(style_sigma, style_mu).sum()\n",
        "          kl_loss = style_kl_loss.item() + content_kl_loss.item()\n",
        "\n",
        "          content_mul_loss = self.mul_loss(content_mul_preds, smoothed_bow.double()).sum()\n",
        "          style_mul_loss = self.mul_loss(style_mul_preds, smoothed_labels.double()).sum()\n",
        "\n",
        "          reconstruction_loss = self.recon_loss(output_dist, input_sequence).sum()\n",
        "\n",
        "          # it = training_numbers['iteration']\n",
        "\n",
        "          lambdas = self.model_hyperparams['lambdas']\n",
        "\n",
        "          ovr_loss = reconstruction_loss + kl_loss + lambdas['lam_style_mul']*style_mul_loss + lambdas['lam_content_mul']*content_mul_loss - lambdas['lam_style_adv']*style_entropy_loss - lambdas['lam_content_adv']*content_entropy_loss\n",
        "\n",
        "\n",
        "          current_batch_size = labels.shape[0]\n",
        "\n",
        "          running_style_adv_loss += style_disc_loss.item()\n",
        "          running_content_adv_loss += content_disc_loss.item()\n",
        "          running_over_loss += ovr_loss.item()\n",
        "          running_recon_loss+= reconstruction_loss.item()\n",
        "          running_content_entropy += content_entropy_loss.item()\n",
        "          running_style_entropy += style_entropy_loss.item()\n",
        "          running_kl_loss += kl_loss\n",
        "          running_content_mul_loss += content_mul_loss.item()\n",
        "          running_style_mul_loss += style_mul_loss.item()\n",
        "\n",
        "        val_style_adv_loss = running_style_adv_loss/(idx+1)\n",
        "        val_content_adv_loss = running_content_adv_loss/(idx+1)\n",
        "        val_over_loss = running_over_loss/(idx+1)\n",
        "        val_recon_loss = running_recon_loss/(idx+1)\n",
        "        val_content_entropy = running_content_entropy/(idx+1)\n",
        "        val_style_entropy = running_style_entropy/(idx+1)\n",
        "        val_kl_loss = running_kl_loss/(idx+1)\n",
        "        val_content_mul_loss = running_content_mul_loss/(idx+1)\n",
        "        val_style_mul_loss = running_style_mul_loss/(idx+1)\n",
        "\n",
        "        # print(\"-----------------------------------------------------------------------\")\n",
        "        val_output_string = \"Overall Loss %9.4f\\n Recon_Loss %9.4f \\n KL-Loss %9.5f\\n Style-Mul-Loss %9.4f\\n Content-Mul-Loss %9.4f\\n Style-Adv-Entropy %9.4f\\n Content-Adv-Entropy %9.4f\\n Content-disc-loss %9.4f\\n Style-disc-loss %9.4f\\n\" % (\n",
        "        val_over_loss, val_recon_loss, val_kl_loss, val_style_mul_loss, val_content_mul_loss, val_style_entropy, val_content_entropy, val_content_adv_loss, val_style_adv_loss)\n",
        "        # print(val_output_string)\n",
        "        val_file.write(val_output_string)\n",
        "        val_file.write('------------------------------------------------------------------')\n",
        "\n",
        "        \n",
        "\n",
        "      val_file.close()\n",
        "\n",
        "      val_losses_dict['e_sal'].append(float(val_style_adv_loss))\n",
        "      val_losses_dict['e_cal'].append(float(val_content_adv_loss))\n",
        "      val_losses_dict['e_sml'].append(float(val_style_mul_loss))\n",
        "      val_losses_dict['e_cml'].append(float(val_content_mul_loss))\n",
        "      val_losses_dict['e_rl'].append(float(val_recon_loss))\n",
        "      val_losses_dict['e_kl'].append(float(val_kl_loss))\n",
        "      val_losses_dict['e_ol'].append(float(val_over_loss))\n",
        "      val_losses_dict['e_se'].append(float(val_style_entropy))\n",
        "      val_losses_dict['e_ce'].append(float(val_content_entropy))\n",
        "\n",
        "      # print(\"Input Sentence\",sentence[0])\n",
        "      # print(\"Input Tokens\", input_sequence[0])\n",
        "      # print(\"Output Tokens\", output_tokens[0])\n",
        "\n",
        "      return val_losses_dict, all_output_tokens, all_sentences, all_labels\n",
        "\n",
        "  def decode_sentence(self, tokens):\n",
        "    set_size = tokens.shape[0]\n",
        "    max_seq_len = tokens.shape[1]\n",
        "    sentences = [[] for i in range(set_size)]\n",
        "    for idx in range(set_size):\n",
        "      sentence_tok = tokens[idx]\n",
        "      for token in sentence_tok:\n",
        "        token = int(token)\n",
        "        if token == 9189:\n",
        "          break\n",
        "        else:\n",
        "          sentences[idx].append(self.index2word[str(token)])\n",
        "      sentences[idx] = \" \".join(sentences[idx])\n",
        "\n",
        "    return sentences #returns (bs) sized list\n",
        "\n",
        "\n",
        "  def save_style_embeddings(self, load_checkpoint, style_embeddings_path):\n",
        "    checkpoint = torch.load(load_checkpoint)\n",
        "    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print('Model Loaded from Checkpoint')\n",
        "    print(\"Collecting Style Embedding Data from Training Set\")\n",
        "    t_n = dict()\n",
        "    t_n['epoch']=1\n",
        "    t_n['iteration']=1\n",
        "    for idx, data in enumerate(tqdm(self.train_dataloader)):\n",
        "          data, sentence = data\n",
        "          output_tuple = self.model(data,t_n)\n",
        "          style_sample = output_tuple[11]\n",
        "          labels = output_tuple[14]\n",
        "          style_embedding_data = torch.cat([style_sample, labels], 1)\n",
        "          with open(style_embeddings_path, \"a\", newline='') as f:\n",
        "            csv_writer = csv.writer(f)\n",
        "            csv_writer.writerows(style_embedding_data.tolist())\n",
        "    return 0\n",
        "    \n",
        "\n",
        "  def test_reconstruction(self, model_load_path, df_savepath):\n",
        "    checkpoint = torch.load(model_load_path)\n",
        "    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print('Model Loaded from Checkpoint')\n",
        "    test_losses_dict = {'e_sal':[], 'e_cal':[], 'e_sml':[], 'e_cml':[], 'e_rl':[], 'e_kl':[], 'e_ol':[], 'e_se':[], 'e_ce':[]}\n",
        "    self.model.eval()\n",
        "    # for idx, data in enumerate(self.test_dataloader):\n",
        "    #   data, sentence, label = \n",
        "    test_losses_dict, output_tokens, input_sentences, orig_label = self.validate(self.test_dataloader, test_losses_dict, test=True)\n",
        "    out_sentences = self.decode_sentence(output_tokens)\n",
        "    df_dict = {'input':input_sentences, 'label':orig_label, 'pred':out_sentences}\n",
        "    df = pd.DataFrame(df_dict)\n",
        "    df.to_csv(df_savepath, index=False)\n",
        "    with open(os.path.join(self.epoch_logging_path, \"TestLosses.json\"),'w') as final_test:\n",
        "          json.dump(test_losses_dict,final_test)\n",
        "\n",
        "\n",
        "  def style_transfer(self, custom_dataloader, target_style_sample, model_load_path, df_savepath):\n",
        "    checkpoint = torch.load(model_load_path)\n",
        "    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print('Model Loaded from Checkpoint')\n",
        "    self.model.eval()\n",
        "    all_labels = list()\n",
        "    all_input_sentences=list()\n",
        "    all_output_tokens = torch.Tensor().to(device)\n",
        "    for idx, data in enumerate(custom_dataloader):\n",
        "      data, sentence, label = data\n",
        "      all_labels.extend(label)\n",
        "      all_input_sentences.extend(sentence)\n",
        "      output_tuple = self.model.inference(data, outtype='style', target_style_sample=target_style_sample)\n",
        "      output_tokens = output_tuple[1]\n",
        "      all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
        "    out_sentences = self.decode_sentence(all_output_tokens)\n",
        "    df_dict = {'input':all_input_sentences, 'label':all_labels, 'pred':out_sentences}\n",
        "    df = pd.DataFrame(df_dict)\n",
        "    df.to_csv(df_savepath, index=False)\n",
        "\n",
        "  def cf_style_transfer(self, custom_dataloader, model_load_path, df_savepath):\n",
        "    cf = self.init_counterfactual()\n",
        "    checkpoint = torch.load(model_load_path)\n",
        "    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print('Model Loaded from Checkpoint')\n",
        "    self.model.eval()\n",
        "    all_labels = list()\n",
        "    all_input_sentences=list()\n",
        "    all_output_tokens = torch.Tensor().to(device)\n",
        "    with torch.no_grad():\n",
        "      for idx, data in enumerate(custom_dataloader):\n",
        "        data, sentence, label = data\n",
        "        all_labels.extend(label)\n",
        "        all_input_sentences.extend(sentence)\n",
        "        output_tuple = self.model.inference(data, outtype='cf_style', cf=cf)\n",
        "        output_tokens = output_tuple[1]\n",
        "        all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
        "        out_sentences = self.decode_sentence(all_output_tokens)\n",
        "        df_dict = {'input':all_input_sentences, 'label':all_labels, 'pred':out_sentences}\n",
        "        df = pd.DataFrame(df_dict)\n",
        "        df.to_csv(df_savepath, index=False)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOcuJz5uS0pq"
      },
      "source": [
        "###Paths to Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRZr5vA8nMPS"
      },
      "outputs": [],
      "source": [
        "train_text_file_path = \"/content/drive/MyDrive/NLPPROJECT/yelp_train_data.txt\"\n",
        "train_labels_file_path = \"/content/drive/MyDrive/NLPPROJECT/yelp_train_labels.txt\"\n",
        "w2i_file_path = \"/content/drive/MyDrive/NLPPROJECT/data/word2index.json\"\n",
        "bow_file_path = \"/content/drive/MyDrive/NLPPROJECT/data/bow.json\"\n",
        "i2w_file_path = \"/content/drive/MyDrive/NLPPROJECT/data/index2word.json\"\n",
        "\n",
        "val_text_file_path = \"/content/drive/MyDrive/NLPPROJECT/val/yelp_train_data.txt\"\n",
        "val_labels_file_path = \"/content/drive/MyDrive/NLPPROJECT/val/yelp_train_labels.txt\"\n",
        "\n",
        "test_text_file_path = \"/content/drive/MyDrive/NLPPROJECT/test/yelp_sampled_data.txt\"\n",
        "test_labels_file_path = \"/content/drive/MyDrive/NLPPROJECT/test/yelp_sampled_labels.txt\"\n",
        "\n",
        "\n",
        "train_dataset = TextDataset(train_text_file_path, train_labels_file_path, w2i_file_path, bow_file_path, i2w_file_path)\n",
        "val_dataset = TextDataset(val_text_file_path, val_labels_file_path, w2i_file_path, bow_file_path, i2w_file_path)\n",
        "test_dataset = TextDataset(test_text_file_path,test_labels_file_path, w2i_file_path, bow_file_path, i2w_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg8Is79d_tho",
        "outputId": "4fd0a944-16df-4cbe-a252-4aefe8fd68e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "444081\n",
            "76404\n",
            "2500\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.__len__())\n",
        "print(val_dataset.__len__())\n",
        "print(test_dataset.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzvCnyndDHhq"
      },
      "source": [
        "###Experiment HyperParams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3brxMmDbDK93"
      },
      "outputs": [],
      "source": [
        "model_hyperparams = dict()\n",
        "hyperparams = dict() \n",
        "model_hyperparams['rnn_hidden_dim'] = 256\n",
        "model_hyperparams['rnn_hidden_dim_combined'] = 512\n",
        "model_hyperparams['style_hidden_dim'] = 8\n",
        "model_hyperparams['content_hidden_dim'] = 128\n",
        "model_hyperparams['vocab_size'] = 9190\n",
        "model_hyperparams['bow_size'] = 8229 #change this according to dataset\n",
        "model_hyperparams['max_seq_length'] = 15\n",
        "model_hyperparams['epochs'] = 20\n",
        "model_hyperparams['batch_size'] = 128\n",
        "model_hyperparams['embedding_size'] = 300\n",
        "model_hyperparams['concat_embedding_dim'] = 136\n",
        "model_hyperparams['num_styles'] = 2 #depends on dataset\n",
        "model_hyperparams['dropout'] = 0.3\n",
        "model_hyperparams['label_smooth'] = 0.1\n",
        "model_hyperparams['weights_path'] = \"/content/drive/MyDrive/NLPPROJECT/data/word_embeddings.npy\"\n",
        "\n",
        "model_hyperparams['lambdas'] = {'lam_style_mul' : 10, 'lam_content_mul':3, 'lam_style_adv':1, \n",
        "           'lam_content_adv' : 0.03}\n",
        "model_hyperparams['kl_params'] = {'kl_weight':0.03, 'kl_max_iter':20000}\n",
        "hyperparams['lrs'] = {'encoder_classifier_lr':0.001,'style_adv_lr':0.001,'content_adv_lr':0.001}\n",
        "hyperparams['eps'] = 1e-8\n",
        "hyperparams['label_smooth'] = 0.1\n",
        "\n",
        "hyperparams['sos_index'] = 1\n",
        "hyperparams['unk_index'] = 2\n",
        "hyperparams['pad_token'] = 0\n",
        "\n",
        "# train_logger = logging.getLogger('train_logger')\n",
        "# val_logger = logging.getLogger('val_logger')\n",
        "# test_logger = logging.getLogger('test_logger')\n",
        "# logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO, filename='/content/drive/MyDrive/NLPPROJECT/app.log', filemode='a')\n",
        "\n",
        "exp_params = dict()\n",
        "exp_params['hyperparams'] = hyperparams\n",
        "exp_params['train_dataset'] = train_dataset\n",
        "exp_params['val_dataset'] = val_dataset\n",
        "exp_params['test_dataset'] = test_dataset\n",
        "# exp_params['train_logger'] = train_logger\n",
        "exp_params['params'] = model_hyperparams\n",
        "exp_params['hyperparams'] = hyperparams\n",
        "exp_params['avg_emb_save_path'] = \"/content/drive/MyDrive/NLPPROJECT/Models/FinalModels/\"\n",
        "exp_params['logging_path'] = \"/content/drive/MyDrive/NLPPROJECT/Logging/\"\n",
        "exp_params['epoch_logging_path'] = \"/content/drive/MyDrive/NLPPROJECT/EpochLogging/\"\n",
        "exp_params['model_save_path'] = \"/content/drive/MyDrive/NLPPROJECT/Models/FinalModels/\"\n",
        "exp_params['index2wordpath'] = \"/content/drive/MyDrive/NLPPROJECT/data/index2word.json\"\n",
        "exp_params['word2indexpath'] = \"/content/drive/MyDrive/NLPPROJECT/data/word2index.json\"\n",
        "exp_params['print_batch'] = 500\n",
        "exp_params['previous_save'] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating Experiment"
      ],
      "metadata": {
        "id": "t1YI-QYD0bXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYL8ETj3HR6x"
      },
      "outputs": [],
      "source": [
        "experiment = Experiment(**exp_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Training VAE"
      ],
      "metadata": {
        "id": "NYXIrqz-0fm0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl7VQiajzXbz"
      },
      "outputs": [],
      "source": [
        "exp_params['previous_save'] = experiment.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Paths for Model Checkpoint and Saving Results"
      ],
      "metadata": {
        "id": "cCElttnJ0kHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQgWZZuL1FFy"
      },
      "outputs": [],
      "source": [
        "model_load_path = exp_params['model_save_path'] + 'model_epoch_teacherforce_20.pt'\n",
        "style_embeddings_path = \"/content/drive/MyDrive/NLPPROJECT/style_embeddings_trainset.csv\"\n",
        "df_savepath_recon = \"/content/drive/MyDrive/NLPPROJECT/test/VAE_Reconstruction.csv\"\n",
        "df_savepath_styletf = \"/content/drive/MyDrive/NLPPROJECT/test/VAE_StyleTransfer.csv\"\n",
        "df_savepath_cfstyletf = \"/content/drive/MyDrive/NLPPROJECT/test/VAE_CF_StyleTransfer.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Saving Style Embeddings"
      ],
      "metadata": {
        "id": "HiInj4kF0pJu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMKbIYXAQF1x"
      },
      "outputs": [],
      "source": [
        "experiment.save_style_embeddings(model_load_path, style_embeddings_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Reconstruction"
      ],
      "metadata": {
        "id": "TERU0RyT0tRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO2UGKLRQHpo"
      },
      "outputs": [],
      "source": [
        "experiment.test_reconstruction(model_load_path, df_savepath_recon)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Loading Saved Embeddings"
      ],
      "metadata": {
        "id": "uvyqn-kv0wzf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csDzfErAa8tC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9048443-dbe8-496a-97fd-42ebae488e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.61920718  4.75216281  4.72622442 -4.6875719   4.73512634  4.70996812\n",
            "  -4.63338785 -4.8545805 ]\n",
            " [-3.39130619 -3.4972019  -3.46859424  3.51854742 -3.5479543  -3.4624991\n",
            "   3.48424762  3.61113937]]\n"
          ]
        }
      ],
      "source": [
        "avg_emb = np.load(\"/content/drive/MyDrive/NLPPROJECT/Models/FinalModels/avg_emb.npy\")\n",
        "custom_dataloader = DataLoader(test_dataset, 1, shuffle=False)\n",
        "print(avg_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Style Transfer"
      ],
      "metadata": {
        "id": "OTbjGMwS02I2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maKo1xZVbMwG",
        "outputId": "9f4b9faa-9aef-4a62-b0f2-f465eee301c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loaded from Checkpoint\n"
          ]
        }
      ],
      "source": [
        "experiment.style_transfer(custom_dataloader, avg_emb, model_load_path, df_savepath_styletf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Style Transfer using Counterfactuals"
      ],
      "metadata": {
        "id": "kMoJMI_U04Rf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozdq5auIGmox"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SDV_MUu-Fzp",
        "outputId": "c5be9822-6171-4dbe-b58a-ef8b13d9bdbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Loaded from Checkpoint\n"
          ]
        }
      ],
      "source": [
        "experiment.cf_style_transfer(custom_dataloader, model_load_path, df_savepath_cfstyletf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gcIYznhUz-F"
      },
      "source": [
        "## Train MLP for Counterfactuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzZfLYwtWmVM"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset\n",
        "class CFDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.inputs = torch.Tensor(self.df.iloc[:,:-2].values.tolist())\n",
        "        self.labels = torch.Tensor(self.df.iloc[:,-2:].values.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.inputs[index], self.labels[index]\n",
        "\n",
        "dataset = CFDataset(model_hyperparams['style_embeddings_path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ug6uCXY1-x-"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 10\n",
        "print_every = 1\n",
        "model_save_path = \"/content/drive/MyDrive/NLPPROJECT/Models/\"\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Create a data loader object\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GljJGgT8GbMk",
        "outputId": "41e259dd-18a8-451a-82f6-4beb5d1f310c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.0709\n",
            "Epoch [2/10], Loss: 0.0315\n",
            "Epoch [3/10], Loss: 0.0588\n",
            "Epoch [4/10], Loss: 0.2049\n",
            "Epoch [5/10], Loss: 0.0311\n",
            "Epoch [6/10], Loss: 0.1279\n",
            "Epoch [7/10], Loss: 0.0187\n",
            "Epoch [8/10], Loss: 0.0149\n",
            "Epoch [9/10], Loss: 0.0486\n",
            "Epoch [10/10], Loss: 0.0957\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = MLP(input_size, hidden_size_1, hidden_size_2, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the loss every few epochs\n",
        "    if (epoch+1) % print_every == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "torch.save({\n",
        "  'model_state_dict': model.state_dict()\n",
        "}, os.path.join(model_save_path, \"mlp\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1k-OxDiGdY4",
        "outputId": "e00d61f5-ba1d-4bd4-8bbb-1ec1a89e37c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_path = \"/content/drive/MyDrive/NLPPROJECT/Models/mlp\"\n",
        "model = MLP(input_size, hidden_size_1, hidden_size_2, output_size)\n",
        "if torch.cuda.is_available():\n",
        "  checkpoint = torch.load(load_path)\n",
        "else:\n",
        "  checkpoint = torch.load(load_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HDft50RGfba"
      },
      "outputs": [],
      "source": [
        "def model_fn(x):\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        x = torch.from_numpy(x).to(torch.float32)\n",
        "        # print(x.shape, x.dtype)\n",
        "        y_pred = torch.nn.functional.softmax(model(x), dim=1).numpy().astype(np.float64)  # use the model to predict labels\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u0YM4OsGoig"
      },
      "outputs": [],
      "source": [
        "cf = Counterfactual(model_fn, shape=(1, 8), target_proba=1.0, target_class=\"other\", lam_init=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sUdWgdpGom_"
      },
      "outputs": [],
      "source": [
        "explanation = cf.explain(dataset.inputs[888159].reshape(1, 8).numpy().astype(np.float64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFOgKuZ4IC6h",
        "outputId": "7e5ba1fd-e81c-4127-ee19-3e434f2d5c39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Explanation(meta={\n",
              "  'name': 'Counterfactual',\n",
              "  'type': ['blackbox', 'tensorflow', 'keras'],\n",
              "  'explanations': ['local'],\n",
              "  'params': {\n",
              "              'shape': (1, 8),\n",
              "              'distance_fn': 'l1',\n",
              "              'target_proba': 1.0,\n",
              "              'target_class': 'other',\n",
              "              'max_iter': 1000,\n",
              "              'early_stop': 50,\n",
              "              'lam_init': 0.1,\n",
              "              'max_lam_steps': 10,\n",
              "              'tol': 0.05,\n",
              "              'learning_rate_init': 0.1,\n",
              "              'eps': 0.01,\n",
              "              'init': 'identity',\n",
              "              'decay': True,\n",
              "              'write_dir': None,\n",
              "              'debug': False,\n",
              "              'feature_range': (-10000000000.0, 10000000000.0),\n",
              "              'is_model': False}\n",
              "            ,\n",
              "  'version': '0.9.2'}\n",
              ", data={\n",
              "  'cf': {\n",
              "          'X': array([[  8.489445 , -12.428226 , -13.7799225, -12.824505 ,   0.1846957,\n",
              "          1.8347408,  -9.834505 , -15.37922  ]], dtype=float32),\n",
              "          'distance': 15.2157621383667,\n",
              "          'lambda': 0.013375000000000001,\n",
              "          'index': 2050,\n",
              "          'class': 0,\n",
              "          'proba': array([[0.95042437, 0.04957561]]),\n",
              "          'loss': 0.20596856137823777}\n",
              "        ,\n",
              "  'all': {\n",
              "           0: [],\n",
              "           1: [],\n",
              "           2: [{'X': array([[  8.472057 , -12.456097 , -13.756281 , -12.659584 ,  -0.5499452,\n",
              "          2.2258444,  -9.796836 , -15.458778 ]], dtype=float32), 'distance': 16.267423629760742, 'lambda': 0.013375000000000001, 'index': 2035, 'class': 0, 'proba': array([[0.95316696, 0.04683299]]), 'loss': 0.2197701245272765}, {'X': array([[  8.4575815 , -12.437709  , -13.767849  , -12.653721  ,\n",
              "         -0.47839215,   2.2193503 ,  -9.782661  , -15.470328  ]],\n",
              "      dtype=float32), 'distance': 16.242259979248047, 'lambda': 0.013375000000000001, 'index': 2036, 'class': 0, 'proba': array([[0.95570338, 0.04429658]]), 'loss': 0.2192024179675324}, {'X': array([[  8.446207  , -12.422616  , -13.787938  , -12.651153  ,\n",
              "         -0.40295646,   2.208971  ,  -9.771781  , -15.478542  ]],\n",
              "      dtype=float32), 'distance': 16.196239471435547, 'lambda': 0.013375000000000001, 'index': 2037, 'class': 0, 'proba': array([[0.95700693, 0.04299307]]), 'loss': 0.21847310688626934}, {'X': array([[  8.437692  , -12.41056   , -13.795846  , -12.651637  ,\n",
              "         -0.32414192,   2.195019  ,  -9.76394   , -15.483685  ]],\n",
              "      dtype=float32), 'distance': 16.144451141357422, 'lambda': 0.013375000000000001, 'index': 2038, 'class': 0, 'proba': array([[0.95805323, 0.04194673]]), 'loss': 0.2176915654251176}, {'X': array([[  8.431804  , -12.401285  , -13.792879  , -12.654948  ,\n",
              "         -0.24238381,   2.1777833 ,  -9.758902  , -15.485999  ]],\n",
              "      dtype=float32), 'distance': 16.06169319152832, 'lambda': 0.013375000000000001, 'index': 2039, 'class': 0, 'proba': array([[0.95887285, 0.04112715]]), 'loss': 0.2165165885164269}, {'X': array([[  8.4283285 , -12.394559  , -13.7802    , -12.6608715 ,\n",
              "         -0.15805987,   2.157529  ,  -9.756446  , -15.485713  ]],\n",
              "      dtype=float32), 'distance': 15.954607009887695, 'lambda': 0.013375000000000001, 'index': 2040, 'class': 0, 'proba': array([[0.95949119, 0.04050877]]), 'loss': 0.21503383217794947}, {'X': array([[  8.427069  , -12.390167  , -13.778433  , -12.669209  ,\n",
              "         -0.07149807,   2.134501  ,  -9.756368  , -15.483034  ]],\n",
              "      dtype=float32), 'distance': 15.841497421264648, 'lambda': 0.013375000000000001, 'index': 2041, 'class': 0, 'proba': array([[0.95986897, 0.04013105]]), 'loss': 0.21349052777627314}, {'X': array([[  8.427843 , -12.387915 , -13.786448 , -12.67977  ,   0.017017 ,\n",
              "          2.1089246,  -9.758476 , -15.478156 ]], dtype=float32), 'distance': 15.746127128601074, 'lambda': 0.013375000000000001, 'index': 2042, 'class': 0, 'proba': array([[0.96003056, 0.03996947]]), 'loss': 0.2122020068206839}, {'X': array([[  8.430484  , -12.387622  , -13.783704  , -12.69238   ,\n",
              "          0.08651085,   2.0810077 ,  -9.762592  , -15.471257  ]],\n",
              "      dtype=float32), 'distance': 15.758988380432129, 'lambda': 0.013375000000000001, 'index': 2043, 'class': 0, 'proba': array([[0.95987469, 0.04012531]]), 'loss': 0.21238651012466056}, {'X': array([[  8.434836  , -12.389125  , -13.771329  , -12.706868  ,\n",
              "          0.13897407,   2.0509436 ,  -9.768549  , -15.462505  ]],\n",
              "      dtype=float32), 'distance': 15.755424499511719, 'lambda': 0.013375000000000001, 'index': 2044, 'class': 0, 'proba': array([[0.95942724, 0.04057273]]), 'loss': 0.21237495173698395}, {'X': array([[  8.440756  , -12.392274  , -13.76974   , -12.723073  ,\n",
              "          0.17619511,   2.0189116 ,  -9.776187  , -15.452056  ]],\n",
              "      dtype=float32), 'distance': 15.718842506408691, 'lambda': 0.013375000000000001, 'index': 2045, 'class': 0, 'proba': array([[0.95864481, 0.04135514]]), 'loss': 0.2119497704832683}, {'X': array([[  8.448108 , -12.39693  , -13.777821 , -12.740838 ,   0.1997834,\n",
              "          1.9850793,  -9.785355 , -15.440057 ]], dtype=float32), 'distance': 15.649579048156738, 'lambda': 0.013375000000000001, 'index': 2046, 'class': 0, 'proba': array([[0.95754242, 0.04245758]]), 'loss': 0.21111576591664927}, {'X': array([[  8.456767  , -12.402966  , -13.794565  , -12.76001   ,\n",
              "          0.21118695,   1.9496033 ,  -9.795906  , -15.426646  ]],\n",
              "      dtype=float32), 'distance': 15.575939178466797, 'lambda': 0.013375000000000001, 'index': 2047, 'class': 0, 'proba': array([[0.95612746, 0.0438726 ]]), 'loss': 0.2102529858593895}, {'X': array([[  8.466616  , -12.410264  , -13.799725  , -12.78044   ,\n",
              "          0.21170975,   1.9126309 ,  -9.807703  , -15.411955  ]],\n",
              "      dtype=float32), 'distance': 15.480583190917969, 'lambda': 0.013375000000000001, 'index': 2048, 'class': 0, 'proba': array([[0.95447034, 0.04552967]]), 'loss': 0.20912575044266732}, {'X': array([[  8.477543  , -12.418717  , -13.794489  , -12.801986  ,\n",
              "          0.20252639,   1.8743001 ,  -9.820612  , -15.396107  ]],\n",
              "      dtype=float32), 'distance': 15.358150482177734, 'lambda': 0.013375000000000001, 'index': 2049, 'class': 0, 'proba': array([[0.95257002, 0.04742996]]), 'loss': 0.20766486559260186}, {'X': array([[  8.489445 , -12.428226 , -13.7799225, -12.824505 ,   0.1846957,\n",
              "          1.8347408,  -9.834505 , -15.37922  ]], dtype=float32), 'distance': 15.2157621383667, 'lambda': 0.013375000000000001, 'index': 2050, 'class': 0, 'proba': array([[0.95042437, 0.04957561]]), 'loss': 0.20596856137823777}],\n",
              "           3: [],\n",
              "           4: [],\n",
              "           5: [],\n",
              "           6: [],\n",
              "           7: [],\n",
              "           8: [],\n",
              "           9: []}\n",
              "         ,\n",
              "  'orig_class': 1,\n",
              "  'orig_proba': 0.9657044410705566,\n",
              "  'success': True}\n",
              ")"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEmsJij3Ldep",
        "outputId": "72d78d3a-7f22-425f-9ec5-039feb9b8eeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.03429553, 0.96570444]])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_fn(dataset.inputs[888159].reshape(1, 8).numpy().astype(np.float64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJd63dB2SXcd"
      },
      "source": [
        "##Data Processing for Human Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = \"/content/drive/MyDrive/NLPPROJECT/test/VAE_CF_StyleTransfer.csv\"\n",
        "path2 = \"/content/drive/MyDrive/NLPPROJECT/test/VAE_StyleTransfer.csv\"\n",
        "path3 = \"/content/drive/MyDrive/NLPPROJECT/test/llm_few_shot_outputs_evaluated_fixed.csv\"\n",
        "path4 = \"/content/drive/MyDrive/NLPPROJECT/test/llm_zero_shot_outputs_evaluated_fixed.csv\"\n",
        "\n",
        "df1 = pd.read_csv(path1)\n",
        "df2 = pd.read_csv(path2)\n",
        "df3 = pd.read_csv(path3)\n",
        "df4 = pd.read_csv(path4)\n",
        "\n",
        "indices = list(np.random.randint(0,710,size=(30,)))\n",
        "print(len(set(indices)))\n",
        "print(indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg9G3mZDXZQO",
        "outputId": "7e05292f-8d2d-4588-cc75-b2d3509ec458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "[509, 46, 550, 576, 527, 271, 6, 623, 9, 291, 44, 285, 103, 6, 6, 295, 115, 692, 250, 402, 199, 271, 483, 362, 583, 257, 357, 207, 505, 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df1 = df1.loc[df1.index[indices]]\n",
        "new_df2 = df2.loc[df1.index[indices]]\n",
        "new_df3 = df3.loc[df1.index[indices]]\n",
        "new_df4 = df4.loc[df1.index[indices]]\n",
        "# new_df2 = df2.sample(n=30, random_state=42, axis=0)\n",
        "# new_df3 = df3.sample(n=30, random_state=42, axis=0)\n",
        "# new_df4 = df4.sample(n=30, random_state=42, axis=0)\n",
        "# print(new_df2.head(3))\n",
        "\n",
        "single_df = new_df2.copy()\n",
        "single_df = single_df.drop(columns=['pred'])\n",
        "single_df['VAE Style Transfer'] = new_df2['pred'] \n",
        "single_df['TS_VAE_ST'] = ''\n",
        "single_df['CP_VAE_ST'] = ''\n",
        "single_df['LQ_VAE_ST'] = ''\n",
        "\n",
        "single_df['VAE CF Style Transfer'] = new_df1['pred']\n",
        "single_df['TS_VAE_CFST'] = ''\n",
        "single_df['CP_VAE_CFST'] = ''\n",
        "single_df['LQ_VAE_CFST'] = ''\n",
        "\n",
        "\n",
        "single_df['Zero Shot LLM'] = new_df4['pred']\n",
        "single_df['TS_LLM_ZS'] = ''\n",
        "single_df['CP_LLM_ZS'] = ''\n",
        "single_df['LQ_LLM_ZS'] = ''\n",
        "\n",
        "single_df['Few Shot LLM'] = new_df3['pred']\n",
        "single_df['TS_LLM_FS'] = ''\n",
        "single_df['CP_LLM_FS'] = ''\n",
        "single_df['LQ_LLM_FS'] = ''\n",
        "\n",
        "print(single_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdoe2CCKZNdd",
        "outputId": "8e8f5b22-f0e0-44e4-d03f-c779814b1717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 input label  \\\n",
            "509  the food and drinks are bad especially for veg...   neg   \n",
            "46       worst experience ever coming to this doctor\\n   neg   \n",
            "550  the hosts seemed to have no clue what they wer...   neg   \n",
            "576             everything was great service is good\\n   pos   \n",
            "527    the moment i found out i had to experience it\\n   pos   \n",
            "\n",
            "           VAE Style Transfer TS_VAE_ST CP_VAE_ST LQ_VAE_ST  \\\n",
            "509    the food and are great                                 \n",
            "46   worst experience to this                                 \n",
            "550    the seemed to be doing                                 \n",
            "576        everything was bad                                 \n",
            "527       the next time i had                                 \n",
            "\n",
            "        VAE CF Style Transfer TS_VAE_CFST CP_VAE_CFST LQ_VAE_CFST  \\\n",
            "509   the food and drinks are                                       \n",
            "46   worst experience to this                                       \n",
            "550    the seemed to be doing                                       \n",
            "576       everything was good                                       \n",
            "527       the next time i had                                       \n",
            "\n",
            "                                         Zero Shot LLM TS_LLM_ZS CP_LLM_ZS  \\\n",
            "509  the food and drinks are good especially for ve...                       \n",
            "46          best experience ever coming to this doctor                       \n",
            "550  the hosts appeared to be knowledgeable on what...                       \n",
            "576       everything was not great service is not good                       \n",
            "527      the moment i found out i had to experience it                       \n",
            "\n",
            "    LQ_LLM_ZS                                       Few Shot LLM TS_LLM_FS  \\\n",
            "509            the food and drinks are good, especially for v...             \n",
            "46                    best experience ever coming to this doctor             \n",
            "550            the hosts seemed to have a great understanding...             \n",
            "576                       everything was terrible service is bad             \n",
            "527                the moment i found out i had to experience it             \n",
            "\n",
            "    CP_LLM_FS LQ_LLM_FS  \n",
            "509                      \n",
            "46                       \n",
            "550                      \n",
            "576                      \n",
            "527                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "single_df.to_csv(\"/content/drive/MyDrive/NLPPROJECT/test/HumanEvaluation_Nithin.csv\")"
      ],
      "metadata": {
        "id": "ywl3QKC7hxiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Human Evaluation Aggregation"
      ],
      "metadata": {
        "id": "JMgGaeue89A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annot1 = \"/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/HumanEvaluation_Nirupan.csv\"\n",
        "annot2 = \"/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/HumanEvaluation_Nithin.csv\"\n",
        "annot3 = \"/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/HumanEvaluation_Shree.csv\"\n",
        "annot4 = \"/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/HumanEvaluation_Adi.csv\"\n",
        "\n",
        "df1 = pd.read_csv(annot1)\n",
        "df2 = pd.read_csv(annot2)\n",
        "df3 = pd.read_csv(annot3)\n",
        "df4 = pd.read_csv(annot4)\n",
        "\n",
        "frames = [df1, df2, df3, df4]\n",
        "final = pd.concat(frames)\n",
        "print(len(final)) #i deleted 3 examples\n",
        "print(final.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O75VzW8w9GW-",
        "outputId": "cc12582d-583c-4f31-ec71-35e7cdd52bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117\n",
            "   Unnamed: 0                                              input label  \\\n",
            "0         529                          rude staff careless vet\\n   neg   \n",
            "1          36                         i loved my food at spago\\n   pos   \n",
            "2         466  it seems like they do not use fresh produce at...   neg   \n",
            "\n",
            "         VAE Style Transfer  TS_VAE_ST  CP_VAE_ST  LQ_VAE_ST  \\\n",
            "0                rude staff          1          3          5   \n",
            "1  i did at this restaurant          2          1          3   \n",
            "2          it will find all          1          1          4   \n",
            "\n",
            "        VAE CF Style Transfer  TS_VAE_CFST  CP_VAE_CFST  LQ_VAE_CFST  \\\n",
            "0                  rude staff          1.0            3            5   \n",
            "1  i loved at this restaurant          1.0            3            4   \n",
            "2           it seems like all          1.0            1            2   \n",
            "\n",
            "                                       Zero Shot LLM  TS_LLM_ZS  CP_LLM_ZS  \\\n",
            "0            staff was polite and vet was attentive.          5          5   \n",
            "1                    i didn't enjoy my food at spago          5          5   \n",
            "2  it seems like they use fresh produce all the time          5          5   \n",
            "\n",
            "   LQ_LLM_ZS                                  Few Shot LLM  TS_LLM_FS  \\\n",
            "0          5                     friendly staff caring vet          5   \n",
            "1          5                      i hated my food at spago          5   \n",
            "2          5  it seems like they use fresh produce at all.          4   \n",
            "\n",
            "   CP_LLM_FS  LQ_LLM_FS  \n",
            "0          5          5  \n",
            "1          5          5  \n",
            "2          5          5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final.columns = ['Index', 'input', 'label', 'VAE Style Transfer', 'TS_VAE_ST',\n",
        "       'CP_VAE_ST', 'LQ_VAE_ST', 'VAE CF Style Transfer', 'TS_VAE_CFST',\n",
        "       'CP_VAE_CFST', 'LQ_VAE_CFST', 'Zero Shot LLM', 'TS_LLM_ZS', 'CP_LLM_ZS',\n",
        "       'LQ_LLM_ZS', 'Few Shot LLM', 'TS_LLM_FS', 'CP_LLM_FS', 'LQ_LLM_FS']"
      ],
      "metadata": {
        "id": "Zq8RNDBl-ZLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_sents = final.Index.unique()\n",
        "val_cnts = final.Index.value_counts()\n",
        "repeated_keys = []\n",
        "for key,value in val_cnts.items():\n",
        "  if value>1:\n",
        "    repeated_keys.append(key)\n",
        "\n",
        "iter_columns = ['TS_VAE_ST', 'CP_VAE_ST', 'LQ_VAE_ST', \n",
        "                  'TS_VAE_CFST', 'CP_VAE_CFST', 'LQ_VAE_CFST', \n",
        "                  'TS_LLM_ZS', 'CP_LLM_ZS', 'LQ_LLM_ZS',\n",
        "                  'TS_LLM_FS', 'CP_LLM_FS', 'LQ_LLM_FS']\n",
        "   \n",
        "for r in repeated_keys:\n",
        "  k = final[final['Index']==r]\n",
        "  row = k.iloc[0].copy()\n",
        "  for col in iter_columns:\n",
        "    row[col] = k[col].mean()\n",
        "  final = final[final['Index']!=r]\n",
        "  final.loc[len(final.index)] = row "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vyWYlV_-Vsm",
        "outputId": "25414c61-d214-4bc0-be58-80722dc7c5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Index                                              input label  \\\n",
            "115     91  the service was also very professional and enj...   pos   \n",
            "\n",
            "                 VAE Style Transfer  TS_VAE_ST  CP_VAE_ST  LQ_VAE_ST  \\\n",
            "115  the service was very enjoyable        1.5        4.0        4.5   \n",
            "\n",
            "              VAE CF Style Transfer  TS_VAE_CFST  CP_VAE_CFST  LQ_VAE_CFST  \\\n",
            "115  the service was very enjoyable          1.5          4.0          4.5   \n",
            "\n",
            "                                     Zero Shot LLM  TS_LLM_ZS  CP_LLM_ZS  \\\n",
            "115  the service was not professional or enjoyable        4.5        4.5   \n",
            "\n",
            "     LQ_LLM_ZS                                       Few Shot LLM  TS_LLM_FS  \\\n",
            "115        4.5  the service was also very unprofessional and u...        4.5   \n",
            "\n",
            "     CP_LLM_FS  LQ_LLM_FS  \n",
            "115        5.0        4.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-1337ac6eccc1>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final.loc[len(final.index)] = row\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final.to_csv('/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/Combined_Human_Eval.csv', index=False)"
      ],
      "metadata": {
        "id": "cmYWvtbxCrhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = pd.read_csv('/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/Combined_Human_Eval.csv')\n",
        "print(final.columns)\n",
        "\n",
        "#for each model, get the average of 3 different scores on the 107 examples, get pos mean, get neg mean. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2WK3zDfC90u",
        "outputId": "761a0bca-56dc-4c22-9987-64ccaa4e1173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Index', 'input', 'label', 'VAE Style Transfer', 'TS_VAE_ST',\n",
            "       'CP_VAE_ST', 'LQ_VAE_ST', 'VAE CF Style Transfer', 'TS_VAE_CFST',\n",
            "       'CP_VAE_CFST', 'LQ_VAE_CFST', 'Zero Shot LLM', 'TS_LLM_ZS', 'CP_LLM_ZS',\n",
            "       'LQ_LLM_ZS', 'Few Shot LLM', 'TS_LLM_FS', 'CP_LLM_FS', 'LQ_LLM_FS'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df = pd.DataFrame(columns = [\"Score Type\", \"VAE Style Transfer\", \"VAE CF Style Transfer\",\"LLM Zero Shot\", \"LLM Few Shot\"])\n",
        "score_types = ['Mean TS Score', 'Mean CP Score', 'Mean LQ Score']\n",
        "for s in score_types:\n",
        "  row=[]\n",
        "  row.append(s)\n",
        "  row.append(final[s[5:7]+'_VAE_ST'].mean())\n",
        "  row.append(final[s[5:7]+'_VAE_CFST'].mean())\n",
        "  row.append(final[s[5:7]+'_LLM_ZS'].mean())\n",
        "  row.append(final[s[5:7]+'_LLM_FS'].mean())\n",
        "  scores_df.loc[len(scores_df.index)] = row \n",
        "\n",
        "print(scores_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDqacWiHFcHv",
        "outputId": "f8c7bfc7-5ceb-4426-ceee-f0b8443aed7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Score Type  VAE Style Transfer  VAE CF Style Transfer  LLM Zero Shot  \\\n",
            "0  Mean TS Score            1.669725               1.402778       4.045872   \n",
            "1  Mean CP Score            2.752294               2.944954       4.577982   \n",
            "2  Mean LQ Score            3.142202               3.275229       4.504587   \n",
            "\n",
            "   LLM Few Shot  \n",
            "0      4.362385  \n",
            "1      4.715596  \n",
            "2      4.637615  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inter_types = ['Mean TS Pos', 'Mean TS Neg', 'Mean CP Pos', 'Mean CP Neg', 'Mean LQ Pos', 'Mean LQ Neg']\n",
        "pos_df = final[final['label']=='pos']\n",
        "neg_df = final[final['label']=='neg']\n",
        "for i in inter_types:\n",
        "  row=[]\n",
        "  row.append(i)\n",
        "  if 'Pos' in i:\n",
        "    row.append(pos_df[i[5:7]+'_VAE_ST'].mean())\n",
        "    row.append(pos_df[i[5:7]+'_VAE_CFST'].mean())\n",
        "    row.append(pos_df[i[5:7]+'_LLM_ZS'].mean())\n",
        "    row.append(pos_df[i[5:7]+'_LLM_FS'].mean())\n",
        "  else:\n",
        "    row.append(neg_df[i[5:7]+'_VAE_ST'].mean())\n",
        "    row.append(neg_df[i[5:7]+'_VAE_CFST'].mean())\n",
        "    row.append(neg_df[i[5:7]+'_LLM_ZS'].mean())\n",
        "    row.append(neg_df[i[5:7]+'_LLM_FS'].mean())\n",
        "  scores_df.loc[len(scores_df.index)] = row \n",
        "\n",
        "print(scores_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P05A0m3wFuLe",
        "outputId": "b73cea8b-2962-49bb-e580-ee0f90e680b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Score Type  VAE Style Transfer  VAE CF Style Transfer  LLM Zero Shot  \\\n",
            "0  Mean TS Score            1.669725               1.402778       4.045872   \n",
            "1  Mean CP Score            2.752294               2.944954       4.577982   \n",
            "2  Mean LQ Score            3.142202               3.275229       4.504587   \n",
            "3    Mean TS Pos            1.607843               1.300000       4.166667   \n",
            "4    Mean TS Neg            1.724138               1.491379       3.939655   \n",
            "5    Mean CP Pos            2.970588               3.343137       4.676471   \n",
            "6    Mean CP Neg            2.560345               2.594828       4.491379   \n",
            "7    Mean LQ Pos            3.372549               3.696078       4.431373   \n",
            "8    Mean LQ Neg            2.939655               2.905172       4.568966   \n",
            "\n",
            "   LLM Few Shot  \n",
            "0      4.362385  \n",
            "1      4.715596  \n",
            "2      4.637615  \n",
            "3      4.637255  \n",
            "4      4.120690  \n",
            "5      4.892157  \n",
            "6      4.560345  \n",
            "7      4.686275  \n",
            "8      4.594828  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df.to_csv('/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/FinalHumanEvalScores.csv', index=False)"
      ],
      "metadata": {
        "id": "1SASXKxTHuA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = pd.read_csv('/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/FinalHumanEvalScores.csv')"
      ],
      "metadata": {
        "id": "zc59hBp-I3-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "  \n",
        "X = ['Mean TS','Mean CP','Mean LQ']\n",
        "VAE_ST = list(scores['VAE Style Transfer'][:3])\n",
        "VAE_CFST = list(scores['VAE CF Style Transfer'][:3])\n",
        "LLM_ZS = list(scores['LLM Zero Shot'][:3])\n",
        "LLM_FS = list(scores['LLM Few Shot'][:3])\n",
        "\n",
        "  \n",
        "X_axis = np.arange(len(X))\n",
        "  \n",
        "plt.bar(X_axis - 0.245, VAE_ST, 0.15, label = 'VAE_ST')\n",
        "plt.bar(X_axis - 0.080, VAE_CFST, 0.15, label = 'VAE_CFST')\n",
        "plt.bar(X_axis + 0.080, LLM_ZS, 0.15, label = 'LLM_ZS')\n",
        "plt.bar(X_axis + 0.245, LLM_FS, 0.15, label = 'LLM_FS')\n",
        "  \n",
        "plt.xticks(X_axis, X)\n",
        "plt.xlabel(\"Score Types\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Human Evaluation Scores\")\n",
        "plt.legend(loc='upper left', prop={'size': 6.7})\n",
        "plt.savefig('/content/drive/MyDrive/NLPPROJECT/HumanEvalFinal/scoresplot.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "rmgUNZVEIl8z",
        "outputId": "40c16829-4f08-4f8d-8ba2-b40b173080cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Characterstics"
      ],
      "metadata": {
        "id": "RG_nJgPo9CwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLYt0XvL997S",
        "outputId": "62e93ab8-94f7-411a-d228-2bed7df4e0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9187\n",
            "3\n",
            "3\n",
            "the\n",
            "the\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/drive/MyDrive/NLPPROJECT/data/embedding.txt\", 'r') as emb:\n",
        "  k = emb.readlines()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/NLPPROJECT/word2index.json\", 'r') as f:\n",
        "  k2 = json.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/NLPPROJECT/data/word2index.json\", 'r') as data:\n",
        "  k3 = json.load(data)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/NLPPROJECT/data/index2word.json\", 'r') as data2:\n",
        "  k4 = json.load(data2)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/NLPPROJECT/index2word.json\", 'r') as data3:\n",
        "  k5 = json.load(data3)\n",
        "\n",
        "\n",
        "print(len(k))\n",
        "print(k2['the'])\n",
        "print(k3['the'])\n",
        "print(k4['3'])\n",
        "print(k5['3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCqE-N-lNzGy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}